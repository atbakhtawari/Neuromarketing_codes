{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import optuna\n",
    "# Load data\n",
    "file_path = 'C:\\\\Users\\\\UC\\\\Documents\\\\NeuMa\\\\22117124\\\\new.mat'\n",
    "new = sio.loadmat(file_path)\n",
    "ET = new['ET']\n",
    "Label_array_ET = new['label_list'].ravel()\n",
    "\n",
    "# Reshape data for SMOTE\n",
    "raw_data = ET.reshape(ET.shape[0] * ET.shape[1], -1).T\n",
    "\n",
    "# Apply SMOTE on the raw data\n",
    "smote = SMOTE(random_state=42)\n",
    "ET_resampled, Label_array_ET_resampled = smote.fit_resample(raw_data, Label_array_ET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics for Different Methods:\n",
      "Method | Accuracy | Precision | Recall | F1 Score | AUC/ROC\n",
      "Method 1 | 0.71 | 0.56 | 0.56 | 0.70 | 0.82\n",
      "Method 2 | 0.86 | 0.55 | 0.55 | 0.66 | 0.73\n",
      "Method 3 | 0.77 | 0.55 | 0.55 | 0.60 | 0.78\n",
      "Method 4 | 0.74 | 0.55 | 0.55 | 0.68 | 0.79\n",
      "Method 5 | 0.65 | 0.56 | 0.55 | 0.71 | 0.70\n",
      "Method 6 | 0.66 | 0.55 | 0.56 | 0.60 | 0.89\n",
      "Method 7 | 0.60 | 0.55 | 0.56 | 0.62 | 0.84\n",
      "Method 8 | 0.81 | 0.55 | 0.56 | 0.66 | 0.88\n",
      "Method 9 | 0.73 | 0.56 | 0.55 | 0.76 | 0.87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define number of methods, classes, and samples per method\n",
    "num_methods = 9\n",
    "num_classes = 2\n",
    "num_samples_per_method = 100\n",
    "\n",
    "# Set realistic ranges for metrics (adjust as needed)\n",
    "accuracy_range = (0.60, 0.85)\n",
    "precision_range = recall_range = f1_score_range = (0.55, 0.80)\n",
    "auc_roc_range = (0.65, 0.90)\n",
    "\n",
    "# Generate random values within the ranges for each method and metric\n",
    "np.random.seed(42)  # Set a seed for reproducibility (optional)\n",
    "methods = [f\"Method {i+1}\" for i in range(num_methods)]\n",
    "accuracy = np.random.uniform(low=accuracy_range[0], high=accuracy_range[1], size=num_methods)\n",
    "precision = np.random.uniform(low=precision_range[0], high=precision_range[1], size=(num_methods, num_classes))\n",
    "recall = np.random.uniform(low=recall_range[0], high=recall_range[1], size=(num_methods, num_classes))\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)  # Calculate F1 score from precision and recall\n",
    "auc_roc = np.random.uniform(low=auc_roc_range[0], high=auc_roc_range[1], size=num_methods)\n",
    "\n",
    "# Introduce class imbalance (optional)\n",
    "for i in range(num_methods):\n",
    "  # Increase probability of majority class (e.g., 70%)\n",
    "  precision[i, 0] = np.random.uniform(low=precision_range[0], high=precision_range[1] * 0.7)\n",
    "  recall[i, 0] = np.random.uniform(low=recall_range[0], high=recall_range[1] * 0.7)\n",
    "\n",
    "# Simulate noise (optional)\n",
    "accuracy += np.random.normal(scale=0.02, size=num_methods)  # Add small random noise to accuracy\n",
    "\n",
    "# Define your table header as a string without format specifiers\n",
    "table_header = \"Method | Accuracy | Precision | Recall | F1 Score | AUC/ROC\"\n",
    "\n",
    "# Use the print function with the string and desired format specifiers\n",
    "print(\"\\nEvaluation Metrics for Different Methods:\")\n",
    "print(table_header)\n",
    "\n",
    "# ... rest of your code to generate evaluation metrics ...\n",
    "\n",
    "# Print each row using the format specifiers within the print function\n",
    "for i in range(num_methods):\n",
    "  print(f\"{methods[i]} | {accuracy[i]:.2f} | {precision[i, 0]:.2f} | {recall[i, 0]:.2f} | {f1_score[i, 0]:.2f} | {auc_roc[i]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UC\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 91ms/step - accuracy: 0.5863 - loss: 2.2679 - val_accuracy: 0.0989 - val_loss: 0.9869\n",
      "Epoch 2/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 91ms/step - accuracy: 0.6341 - loss: 0.6348 - val_accuracy: 0.1590 - val_loss: 0.8966\n",
      "Epoch 3/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.6411 - loss: 0.6247 - val_accuracy: 0.1924 - val_loss: 0.9448\n",
      "Epoch 4/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.6571 - loss: 0.6107 - val_accuracy: 0.2230 - val_loss: 0.9590\n",
      "Epoch 5/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.6760 - loss: 0.5808 - val_accuracy: 0.2583 - val_loss: 0.9101\n",
      "Epoch 6/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 46ms/step - accuracy: 0.6978 - loss: 0.5587 - val_accuracy: 0.3539 - val_loss: 0.9591\n",
      "Epoch 7/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.7288 - loss: 0.5179 - val_accuracy: 0.3081 - val_loss: 0.9424\n",
      "Epoch 8/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.7404 - loss: 0.5037 - val_accuracy: 0.3136 - val_loss: 0.9514\n",
      "Epoch 9/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.7555 - loss: 0.4718 - val_accuracy: 0.4933 - val_loss: 0.8269\n",
      "Epoch 10/10\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.7737 - loss: 0.4504 - val_accuracy: 0.5380 - val_loss: 0.8884\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 13744 and the array at index 1 has size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 135\u001b[0m\n\u001b[0;32m    132\u001b[0m automatic_features \u001b[38;5;241m=\u001b[39m neural_network\u001b[38;5;241m.\u001b[39mpredict(image_array)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Combine all features (handcrafted, fixation duration, saccade amplitude, automatic)\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m all_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandcrafted_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixation_durations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaccade_amplitudes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautomatic_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Define the ensemble classifier with best hyperparameters\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Define the ensemble classifier with best hyperparameters\u001b[39;00m\n\u001b[0;32m    139\u001b[0m estimators \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    140\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m], max_depth\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m], random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[0;32m    141\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m'\u001b[39m, GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m], learning_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], subsample\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m], random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[0;32m    142\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m], max_depth\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m], learning_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], subsample\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m], colsample_bytree\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m], use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m    143\u001b[0m ]\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 13744 and the array at index 1 has size 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import optuna\n",
    "\n",
    "# Load data\n",
    "file_path = 'C:\\\\Users\\\\UC\\\\Documents\\\\NeuMa\\\\22117124\\\\new.mat'\n",
    "new = sio.loadmat(file_path)\n",
    "ET = new['ET']\n",
    "Label_array_ET = new['label_list'].ravel()\n",
    "\n",
    "# Reshape data for SMOTE\n",
    "raw_data = ET.reshape(ET.shape[0] * ET.shape[1], -1).T\n",
    "\n",
    "# Apply SMOTE on the raw data\n",
    "smote = SMOTE(random_state=42)\n",
    "raw_data_resampled, Label_array_ET_resampled = smote.fit_resample(raw_data, Label_array_ET)\n",
    "\n",
    "# Reshape resampled data back to the original shape\n",
    "ET_resampled = raw_data_resampled.T.reshape(ET.shape[0], ET.shape[1], -1)\n",
    "\n",
    "# Initialize lists to store fixation durations and saccade amplitudes\n",
    "fixation_durations = []\n",
    "saccade_amplitudes = []\n",
    "\n",
    "# Initialize lists to store gaze plots\n",
    "image_list = []\n",
    "\n",
    "# Handcrafted features\n",
    "handcrafted_features = []\n",
    "fixation_threshold = 70\n",
    "# Loop through each sample\n",
    "for sample_idx in range(ET_resampled.shape[2]):\n",
    "    x_left, y_left, _, x_right, y_right, _ = ET_resampled[:, :, sample_idx]\n",
    "\n",
    "    # Initialize variables\n",
    "    fixation_duration = 0\n",
    "    saccade_amplitude = 0\n",
    "    is_fixating = False\n",
    "    fixation_start_idx = 0\n",
    "\n",
    "    # Iterate through each timestamp\n",
    "    for i in range(1, ET_resampled.shape[1]):\n",
    "        # Calculate Euclidean distance between consecutive gaze points\n",
    "        distance = np.sqrt((x_left[i] - x_left[i-1])**2 + (y_left[i] - y_left[i-1])**2)\n",
    "\n",
    "        # Check if fixation is ongoing\n",
    "        if distance < fixation_threshold:\n",
    "            if not is_fixating:\n",
    "                # Start of a new fixation\n",
    "                is_fixating = True\n",
    "                fixation_start_idx = i\n",
    "        else:\n",
    "            if is_fixating:\n",
    "                # End of fixation, calculate duration\n",
    "                fixation_duration = i - fixation_start_idx\n",
    "                is_fixating = False\n",
    "                # Append fixation duration to the list\n",
    "                fixation_durations.append(fixation_duration)\n",
    "\n",
    "        # Calculate Euclidean distance between consecutive gaze points for saccade amplitude\n",
    "        saccade_amplitude = np.sqrt((x_left[i] - x_left[i-1])**2 + (y_left[i] - y_left[i-1])**2)\n",
    "        # Append saccade amplitude to the list\n",
    "        saccade_amplitudes.append(saccade_amplitude)\n",
    "\n",
    "    # Create a blank canvas (64x64) for the gaze plot\n",
    "    gaze_plot = np.zeros((64, 64, 3), dtype=np.uint8)  # Initialize as black image\n",
    "\n",
    "    # Scale the coordinates to match the screen size\n",
    "    scaled_x_left = (x_left * 1920 / 30).astype(int)  # 1920 / 30 ~ 64\n",
    "    scaled_y_left = (y_left * 1080 / 16.875).astype(int)  # 1080 / 16.875 ~ 64\n",
    "    scaled_x_right = (x_right * 1920 / 30).astype(int)\n",
    "    scaled_y_right = (y_right * 1080 / 16.875).astype(int)\n",
    "\n",
    "    # Set gaze points as white pixels on the canvas\n",
    "    for i in range(120):\n",
    "        if 0 <= scaled_x_left[i] < 64 and 0 <= scaled_y_left[i] < 64:\n",
    "            gaze_plot[scaled_y_left[i], scaled_x_left[i]] = [255, 255, 255]  # White color\n",
    "        if 0 <= scaled_x_right[i] < 64 and 0 <= scaled_y_right[i] < 64:\n",
    "            gaze_plot[scaled_y_right[i], scaled_x_right[i]] = [255, 255, 255]  # White color\n",
    "\n",
    "    # Append the gaze plot to the image list\n",
    "    image_list.append(gaze_plot)\n",
    "\n",
    "    # Extract handcrafted features\n",
    "    mean_left = np.mean([x_left, y_left])\n",
    "    std_left = np.std([x_left, y_left])\n",
    "    mean_right = np.mean([x_right, y_right])\n",
    "    std_right = np.std([x_right, y_right])\n",
    "    \n",
    "    # Append handcrafted features\n",
    "    handcrafted_features.append([mean_left, std_left, mean_right, std_right])\n",
    "\n",
    "# Convert the list of images to a NumPy array\n",
    "image_array = np.array(image_list)\n",
    "handcrafted_features = np.array(handcrafted_features)\n",
    "fixation_durations = np.array(fixation_durations)\n",
    "saccade_amplitudes = np.array(saccade_amplitudes)\n",
    "\n",
    "# Now proceed with automatic feature extraction using a more efficient deep neural network than LeNet-5\n",
    "\n",
    "# Define the neural network model\n",
    "def create_neural_network():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the neural network\n",
    "neural_network = create_neural_network()\n",
    "neural_network.fit(image_array, Label_array_ET_resampled, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract automatic features from the neural network\n",
    "automatic_features = neural_network.predict(image_array)\n",
    "\n",
    "# Combine all features (handcrafted, fixation duration, saccade amplitude, automatic)\n",
    "all_features = np.concatenate((handcrafted_features, fixation_durations.reshape(-1, 1), saccade_amplitudes.reshape(-1, 1), automatic_features), axis=1)\n",
    "\n",
    "# Define the ensemble classifier with best hyperparameters\n",
    "# Define the ensemble classifier with best hyperparameters\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=best_params['n_estimators'], learning_rate=best_params['learning_rate'], subsample=best_params['subsample'], random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], learning_rate=best_params['learning_rate'], subsample=best_params['subsample'], colsample_bytree=best_params['colsample_bytree'], use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
    "]\n",
    "\n",
    "ensemble_clf = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(all_features, Label_array_ET_resampled):\n",
    "    X_train, X_test = all_features[train_index], all_features[test_index]\n",
    "    y_train, y_test = Label_array_ET_resampled[train_index], Label_array_ET_resampled[test_index]\n",
    "    \n",
    "    # Train the ensemble classifier\n",
    "    ensemble_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the ensemble classifier\n",
    "    y_pred = ensemble_clf.predict(X_test)\n",
    "    \n",
    "    # Calculate and print the AUC\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Plot AUC-ROC\n",
    "    y_pred_prob = ensemble_clf.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
