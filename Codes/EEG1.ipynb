{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      1409\n",
      "           1       0.71      0.60      0.65      1340\n",
      "\n",
      "    accuracy                           0.69      2749\n",
      "   macro avg       0.69      0.68      0.68      2749\n",
      "weighted avg       0.69      0.69      0.68      2749\n",
      "\n",
      "{'gb__n_estimators': 50, 'rf__n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      1409\n",
      "           1       0.72      0.59      0.65      1340\n",
      "\n",
      "    accuracy                           0.69      2749\n",
      "   macro avg       0.69      0.69      0.69      2749\n",
      "weighted avg       0.69      0.69      0.69      2749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load the data\n",
    "file_path = 'C:\\\\Users\\\\UC\\\\Documents\\\\NeuMa\\\\22117124\\\\new.mat'\n",
    "new = sio.loadmat(file_path)\n",
    "Label = new['label_list'].flatten()\n",
    "EEG = new['EEG']\n",
    "\n",
    "# Feature extraction for ML\n",
    "def extract_ml_features(data):\n",
    "    features = []\n",
    "    for i in range(data.shape[2]):  # Iterate over samples\n",
    "        sample = data[:, :, i]\n",
    "        sample_features = []\n",
    "        for j in range(data.shape[0]):  # Iterate over channels\n",
    "            channel_data = sample[j, :]\n",
    "            # Statistical features\n",
    "            mean = np.mean(channel_data)\n",
    "            var = np.var(channel_data)\n",
    "            skewness = skew(channel_data)\n",
    "            kurt = kurtosis(channel_data)\n",
    "            # Frequency domain features using Welch's method\n",
    "            freqs, psd = welch(channel_data)\n",
    "            psd_mean = np.mean(psd)\n",
    "            psd_std = np.std(psd)\n",
    "            # Combine all features\n",
    "            sample_features.extend([mean, var, skewness, kurt, psd_mean, psd_std])\n",
    "        features.append(sample_features)\n",
    "    return np.array(features)\n",
    "\n",
    "ml_features = extract_ml_features(EEG)\n",
    "\n",
    "# Feature extraction for DL using CNN\n",
    "input_shape = (EEG.shape[0], EEG.shape[1], 1)\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "flatten = Flatten()(pool2)\n",
    "dense1 = Dense(128, activation='relu')(flatten)\n",
    "output_layer = Dense(64, activation='relu')(dense1)  # Output for feature extraction\n",
    "\n",
    "cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Reshape data for CNN input\n",
    "data_cnn = EEG.reshape(EEG.shape[2], EEG.shape[0], EEG.shape[1], 1)\n",
    "\n",
    "cnn_features = cnn_model.predict(data_cnn)\n",
    "\n",
    "# Combine ML and DL features\n",
    "combined_features = np.concatenate((ml_features, cnn_features), axis=1)\n",
    "\n",
    "# Handle imbalanced data using SMOTE\n",
    "smote = SMOTE()\n",
    "combined_features_resampled, labels_resampled = smote.fit_resample(combined_features, Label)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features_resampled, labels_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train ensemble classifiers\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Custom Ensemble (Voting)\n",
    "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('gb', gb_clf)], voting='soft')\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble classifier\n",
    "y_pred = ensemble_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'gb__n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=ensemble_clf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 23:11:20,870] A new study created in memory with name: no-name-457f96df-a829-4a48-92be-92b86714731f\n",
      "[I 2024-05-19 23:55:56,765] Trial 0 finished with value: 0.7169879956347763 and parameters: {'rf_n_estimators': 283, 'gb_n_estimators': 276, 'xgb_n_estimators': 172}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 00:15:01,426] Trial 1 finished with value: 0.7169879956347763 and parameters: {'rf_n_estimators': 292, 'gb_n_estimators': 122, 'xgb_n_estimators': 224}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 00:22:26,178] Trial 2 finished with value: 0.6911604219716261 and parameters: {'rf_n_estimators': 86, 'gb_n_estimators': 52, 'xgb_n_estimators': 80}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 00:45:58,749] Trial 3 finished with value: 0.7093488541287741 and parameters: {'rf_n_estimators': 199, 'gb_n_estimators': 176, 'xgb_n_estimators': 293}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 01:22:44,915] Trial 4 finished with value: 0.682429974536195 and parameters: {'rf_n_estimators': 69, 'gb_n_estimators': 284, 'xgb_n_estimators': 150}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 01:57:00,805] Trial 5 finished with value: 0.7151691524190614 and parameters: {'rf_n_estimators': 294, 'gb_n_estimators': 260, 'xgb_n_estimators': 174}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 02:26:56,132] Trial 6 finished with value: 0.7038923244816296 and parameters: {'rf_n_estimators': 172, 'gb_n_estimators': 245, 'xgb_n_estimators': 174}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 02:40:06,310] Trial 7 finished with value: 0.7093488541287741 and parameters: {'rf_n_estimators': 222, 'gb_n_estimators': 84, 'xgb_n_estimators': 91}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 03:15:01,701] Trial 8 finished with value: 0.7068024736267734 and parameters: {'rf_n_estimators': 242, 'gb_n_estimators': 275, 'xgb_n_estimators': 261}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 03:40:20,750] Trial 9 finished with value: 0.6929792651873409 and parameters: {'rf_n_estimators': 160, 'gb_n_estimators': 204, 'xgb_n_estimators': 55}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 04:07:05,165] Trial 10 finished with value: 0.691524190614769 and parameters: {'rf_n_estimators': 128, 'gb_n_estimators': 221, 'xgb_n_estimators': 131}. Best is trial 0 with value: 0.7169879956347763.\n",
      "[I 2024-05-20 04:26:15,336] Trial 11 finished with value: 0.7213532193524919 and parameters: {'rf_n_estimators': 296, 'gb_n_estimators': 122, 'xgb_n_estimators': 223}. Best is trial 11 with value: 0.7213532193524919.\n",
      "[I 2024-05-20 04:47:15,535] Trial 12 finished with value: 0.7155329210622045 and parameters: {'rf_n_estimators': 256, 'gb_n_estimators': 146, 'xgb_n_estimators': 225}. Best is trial 11 with value: 0.7213532193524919.\n",
      "[I 2024-05-20 05:05:34,552] Trial 13 finished with value: 0.7238995998544926 and parameters: {'rf_n_estimators': 267, 'gb_n_estimators': 117, 'xgb_n_estimators': 215}. Best is trial 13 with value: 0.7238995998544926.\n",
      "[I 2024-05-20 05:30:03,514] Trial 14 finished with value: 0.7206256820662059 and parameters: {'rf_n_estimators': 254, 'gb_n_estimators': 116, 'xgb_n_estimators': 224}. Best is trial 13 with value: 0.7238995998544926.\n",
      "[I 2024-05-20 05:55:27,518] Trial 15 finished with value: 0.7115314659876318 and parameters: {'rf_n_estimators': 211, 'gb_n_estimators': 157, 'xgb_n_estimators': 247}. Best is trial 13 with value: 0.7238995998544926.\n",
      "[I 2024-05-20 06:13:23,956] Trial 16 finished with value: 0.7228082939250636 and parameters: {'rf_n_estimators': 267, 'gb_n_estimators': 93, 'xgb_n_estimators': 199}. Best is trial 13 with value: 0.7238995998544926.\n",
      "[I 2024-05-20 06:26:53,206] Trial 17 finished with value: 0.7078937795562023 and parameters: {'rf_n_estimators': 128, 'gb_n_estimators': 79, 'xgb_n_estimators': 197}. Best is trial 13 with value: 0.7238995998544926.\n",
      "[I 2024-05-20 06:44:48,905] Trial 18 finished with value: 0.7311749727173518 and parameters: {'rf_n_estimators': 265, 'gb_n_estimators': 89, 'xgb_n_estimators': 300}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 06:56:26,947] Trial 19 finished with value: 0.7260822117133503 and parameters: {'rf_n_estimators': 224, 'gb_n_estimators': 58, 'xgb_n_estimators': 294}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 07:10:55,984] Trial 20 finished with value: 0.7235358312113496 and parameters: {'rf_n_estimators': 237, 'gb_n_estimators': 77, 'xgb_n_estimators': 292}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 07:22:48,576] Trial 21 finished with value: 0.7206256820662059 and parameters: {'rf_n_estimators': 195, 'gb_n_estimators': 61, 'xgb_n_estimators': 269}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 07:40:56,558] Trial 22 finished with value: 0.7217169879956348 and parameters: {'rf_n_estimators': 230, 'gb_n_estimators': 107, 'xgb_n_estimators': 299}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 08:05:05,000] Trial 23 finished with value: 0.7129865405602037 and parameters: {'rf_n_estimators': 266, 'gb_n_estimators': 150, 'xgb_n_estimators': 273}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 08:18:55,696] Trial 24 finished with value: 0.7235358312113496 and parameters: {'rf_n_estimators': 270, 'gb_n_estimators': 65, 'xgb_n_estimators': 248}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 08:36:23,258] Trial 25 finished with value: 0.7220807566387777 and parameters: {'rf_n_estimators': 191, 'gb_n_estimators': 98, 'xgb_n_estimators': 279}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 09:07:15,198] Trial 26 finished with value: 0.7224445252819207 and parameters: {'rf_n_estimators': 216, 'gb_n_estimators': 130, 'xgb_n_estimators': 247}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 13:13:50,125] Trial 27 finished with value: 0.7217169879956348 and parameters: {'rf_n_estimators': 249, 'gb_n_estimators': 182, 'xgb_n_estimators': 196}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 13:28:00,490] Trial 28 finished with value: 0.7162604583484904 and parameters: {'rf_n_estimators': 276, 'gb_n_estimators': 67, 'xgb_n_estimators': 118}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 13:42:53,877] Trial 29 finished with value: 0.7035285558384867 and parameters: {'rf_n_estimators': 148, 'gb_n_estimators': 99, 'xgb_n_estimators': 151}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 13:53:55,465] Trial 30 finished with value: 0.7180793015642052 and parameters: {'rf_n_estimators': 233, 'gb_n_estimators': 51, 'xgb_n_estimators': 254}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 14:08:32,263] Trial 31 finished with value: 0.7195343761367771 and parameters: {'rf_n_estimators': 241, 'gb_n_estimators': 80, 'xgb_n_estimators': 286}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 14:24:14,789] Trial 32 finished with value: 0.7238995998544926 and parameters: {'rf_n_estimators': 280, 'gb_n_estimators': 81, 'xgb_n_estimators': 293}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 14:49:45,756] Trial 33 finished with value: 0.7268097489996362 and parameters: {'rf_n_estimators': 285, 'gb_n_estimators': 131, 'xgb_n_estimators': 275}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 15:30:46,795] Trial 34 finished with value: 0.7129865405602037 and parameters: {'rf_n_estimators': 285, 'gb_n_estimators': 162, 'xgb_n_estimators': 233}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 17:20:41,894] Trial 35 finished with value: 0.7217169879956348 and parameters: {'rf_n_estimators': 265, 'gb_n_estimators': 134, 'xgb_n_estimators': 273}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 17:42:21,067] Trial 36 finished with value: 0.7151691524190614 and parameters: {'rf_n_estimators': 296, 'gb_n_estimators': 109, 'xgb_n_estimators': 206}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 18:01:09,229] Trial 37 finished with value: 0.7137140778464897 and parameters: {'rf_n_estimators': 206, 'gb_n_estimators': 126, 'xgb_n_estimators': 262}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 18:27:07,631] Trial 38 finished with value: 0.7184430702073481 and parameters: {'rf_n_estimators': 283, 'gb_n_estimators': 93, 'xgb_n_estimators': 282}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 18:47:21,892] Trial 39 finished with value: 0.7271735176427792 and parameters: {'rf_n_estimators': 254, 'gb_n_estimators': 140, 'xgb_n_estimators': 236}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 19:11:06,502] Trial 40 finished with value: 0.7133503092033466 and parameters: {'rf_n_estimators': 221, 'gb_n_estimators': 185, 'xgb_n_estimators': 240}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-20 19:36:45,071] Trial 41 finished with value: 0.7151691524190614 and parameters: {'rf_n_estimators': 255, 'gb_n_estimators': 141, 'xgb_n_estimators': 300}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 01:29:50,458] Trial 42 finished with value: 0.7042560931247727 and parameters: {'rf_n_estimators': 249, 'gb_n_estimators': 111, 'xgb_n_estimators': 210}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 01:47:51,702] Trial 43 finished with value: 0.7209894507093488 and parameters: {'rf_n_estimators': 183, 'gb_n_estimators': 131, 'xgb_n_estimators': 259}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 02:11:25,812] Trial 44 finished with value: 0.709712622771917 and parameters: {'rf_n_estimators': 287, 'gb_n_estimators': 159, 'xgb_n_estimators': 267}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 02:36:55,073] Trial 45 finished with value: 0.7268097489996362 and parameters: {'rf_n_estimators': 300, 'gb_n_estimators': 173, 'xgb_n_estimators': 282}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 03:04:31,982] Trial 46 finished with value: 0.7297198981447799 and parameters: {'rf_n_estimators': 294, 'gb_n_estimators': 195, 'xgb_n_estimators': 285}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 03:51:19,043] Trial 47 finished with value: 0.710440160058203 and parameters: {'rf_n_estimators': 299, 'gb_n_estimators': 222, 'xgb_n_estimators': 285}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 04:23:33,547] Trial 48 finished with value: 0.7242633684976355 and parameters: {'rf_n_estimators': 300, 'gb_n_estimators': 205, 'xgb_n_estimators': 279}. Best is trial 18 with value: 0.7311749727173518.\n",
      "[I 2024-05-21 04:43:50,693] Trial 49 finished with value: 0.6995271007639141 and parameters: {'rf_n_estimators': 61, 'gb_n_estimators': 167, 'xgb_n_estimators': 236}. Best is trial 18 with value: 0.7311749727173518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'rf_n_estimators': 265, 'gb_n_estimators': 89, 'xgb_n_estimators': 300}\n",
      "Optimized model evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      1409\n",
      "           1       0.75      0.66      0.70      1340\n",
      "\n",
      "    accuracy                           0.73      2749\n",
      "   macro avg       0.73      0.72      0.72      2749\n",
      "weighted avg       0.73      0.73      0.72      2749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, LSTM\n",
    "import optuna\n",
    "import pywt\n",
    "from xgboost import XGBClassifier  # Make sure to import XGBClassifier\n",
    "\n",
    "# Load the data\n",
    "file_path = 'C:\\\\Users\\\\UC\\\\Documents\\\\NeuMa\\\\22117124\\\\new.mat'\n",
    "new = sio.loadmat(file_path)\n",
    "Label = new['label_list'].flatten()\n",
    "EEG = new['EEG']\n",
    "\n",
    "# Feature extraction for ML\n",
    "def extract_ml_features(data):\n",
    "    features = []\n",
    "    for i in range(data.shape[2]):  # Iterate over samples\n",
    "        sample = data[:, :, i]\n",
    "        sample_features = []\n",
    "        for j in range(data.shape[0]):  # Iterate over channels\n",
    "            channel_data = sample[j, :]\n",
    "            # Statistical features\n",
    "            mean = np.mean(channel_data)\n",
    "            var = np.var(channel_data)\n",
    "            skewness = skew(channel_data)\n",
    "            kurt = kurtosis(channel_data)\n",
    "            # Frequency domain features using Welch's method\n",
    "            freqs, psd = welch(channel_data)\n",
    "            psd_mean = np.mean(psd)\n",
    "            psd_std = np.std(psd)\n",
    "            # Combine all features\n",
    "            sample_features.extend([mean, var, skewness, kurt, psd_mean, psd_std])\n",
    "        features.append(sample_features)\n",
    "    return np.array(features)\n",
    "\n",
    "# Wavelet transform features\n",
    "def extract_wavelet_features(data):\n",
    "    features = []\n",
    "    for i in range(data.shape[2]):  # Iterate over samples\n",
    "        sample = data[:, :, i]\n",
    "        sample_features = []\n",
    "        for j in range(data.shape[0]):  # Iterate over channels\n",
    "            channel_data = sample[j, :]\n",
    "            coeffs = pywt.wavedec(channel_data, 'db4', level=4)\n",
    "            for coeff in coeffs:\n",
    "                sample_features.extend([np.mean(coeff), np.std(coeff)])\n",
    "        features.append(sample_features)\n",
    "    return np.array(features)\n",
    "\n",
    "ml_features = extract_ml_features(EEG)\n",
    "wavelet_features = extract_wavelet_features(EEG)\n",
    "ml_features = np.concatenate((ml_features, wavelet_features), axis=1)\n",
    "\n",
    "# Feature extraction for DL using CNN and LSTM\n",
    "input_shape = (EEG.shape[0], EEG.shape[1], 1)\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "flatten = Flatten()(pool2)\n",
    "\n",
    "# Reshape for LSTM layer\n",
    "reshape_layer = Reshape((flatten.shape[1], 1))(flatten)\n",
    "lstm_layer = LSTM(64)(reshape_layer)\n",
    "dense1 = Dense(128, activation='relu')(lstm_layer)\n",
    "output_layer = Dense(64, activation='relu')(dense1)  # Output for feature extraction\n",
    "\n",
    "cnn_rnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "cnn_rnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Reshape data for CNN input\n",
    "data_cnn = EEG.reshape(EEG.shape[2], EEG.shape[0], EEG.shape[1], 1)\n",
    "\n",
    "cnn_rnn_features = cnn_rnn_model.predict(data_cnn)\n",
    "\n",
    "# Combine ML and DL features\n",
    "combined_features = np.concatenate((ml_features, cnn_rnn_features), axis=1)\n",
    "\n",
    "# Handle imbalanced data using SMOTE\n",
    "smote = SMOTE()\n",
    "combined_features_resampled, labels_resampled = smote.fit_resample(combined_features, Label)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features_resampled, labels_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Advanced Ensemble with Stacking and Hyperparameter Optimization\n",
    "def objective(trial):\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 300)\n",
    "    gb_n_estimators = trial.suggest_int('gb_n_estimators', 50, 300)\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300)\n",
    "\n",
    "    rf_clf = RandomForestClassifier(n_estimators=rf_n_estimators)\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=gb_n_estimators)\n",
    "    xgb_clf = XGBClassifier(n_estimators=xgb_n_estimators)\n",
    "\n",
    "    stacking_clf = StackingClassifier(estimators=[\n",
    "        ('rf', rf_clf), \n",
    "        ('gb', gb_clf),\n",
    "        ('xgb', xgb_clf)\n",
    "    ], final_estimator=RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(\"Best parameters found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the best model found by Optuna\n",
    "best_params = study.best_params\n",
    "rf_clf = RandomForestClassifier(n_estimators=best_params['rf_n_estimators'])\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=best_params['gb_n_estimators'])\n",
    "xgb_clf = XGBClassifier(n_estimators=best_params['xgb_n_estimators'])\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=[\n",
    "    ('rf', rf_clf), \n",
    "    ('gb', gb_clf),\n",
    "    ('xgb', xgb_clf)\n",
    "], final_estimator=RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "y_pred_optimized = stacking_clf.predict(X_test)\n",
    "print(\"Optimized model evaluation:\")\n",
    "print(classification_report(y_test, y_pred_optimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
